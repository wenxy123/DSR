{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8c8a5c-0dd2-4ee8-ba2f-44738e9bc94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import vtk\n",
    "import vtk.numpy_interface.dataset_adapter as dsa\n",
    "wdo = dsa.WrapDataObject\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vtk.util.numpy_support import vtk_to_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9110ad-8b38-4500-a97c-0bffdc851c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split, SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.ndimage import zoom  # For resampling\n",
    "import math \n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.linalg import vector_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55879606-0927-4f66-9425-223919436442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dfed51-f348-4b83-9fbb-2380318047b9",
   "metadata": {},
   "source": [
    "The dataset used for pre-training consists of CFD data. For each observation time of each patient, the data contain n points, comprising two components:\n",
    "\n",
    "- (i) velocity vectors of shape (n,3);\n",
    "- (ii) corresponding spatial coordinates of shape (n,3).\n",
    "\n",
    "The following code illustrates how to sample 100 points from a single patient and transform the point data into a cubic patch (of shape (N, 3, 16, 16, 16)) for super-resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488fa32f-5bf5-41ea-88e1-6713c017ad56",
   "metadata": {},
   "source": [
    "## Step 1: Generate training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "118ff1fc-76f8-43e2-91d7-fd295e37f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_point(args):\n",
    "    idx, sample_all_data, sample_all_data_point, all_data, all_data_point, epsilon, tree, fixed_grid_size = args\n",
    "    point = sample_all_data[idx]\n",
    "    indices = tree.query_ball_point(point, r=epsilon)\n",
    "    if idx not in indices:\n",
    "        indices.append(idx)\n",
    "    selected_points = all_data[indices]\n",
    "    num_neighbors = len(indices)\n",
    "    t = max(1, int(math.log2(num_neighbors) // 3))\n",
    "\n",
    "    x_min, y_min, z_min = selected_points.min(axis=0)\n",
    "    x_max, y_max, z_max = selected_points.max(axis=0)\n",
    "\n",
    "    num_partitions = 2 ** t\n",
    "    x_edges = np.linspace(x_min, x_max, num_partitions + 1)\n",
    "    y_edges = np.linspace(y_min, y_max, num_partitions + 1)\n",
    "    z_edges = np.linspace(z_min, z_max, num_partitions + 1)\n",
    "    x_centers = (x_edges[:-1] + x_edges[1:]) / 2\n",
    "    y_centers = (y_edges[:-1] + y_edges[1:]) / 2\n",
    "    z_centers = (z_edges[:-1] + z_edges[1:]) / 2\n",
    "    Xc, Yc, Zc = np.meshgrid(\n",
    "        x_centers, y_centers, z_centers, indexing='ij'\n",
    "    )\n",
    "    centers = np.column_stack((Xc.ravel(), Yc.ravel(), Zc.ravel()))\n",
    "        \n",
    "    channel_x = np.zeros(centers.shape[0])\n",
    "    channel_y = np.zeros(centers.shape[0])\n",
    "    channel_z = np.zeros(centers.shape[0])\n",
    "    \n",
    "    selected_values = all_data_point[indices]    \n",
    "\n",
    "    for i, center in enumerate(centers):\n",
    "        distances = np.linalg.norm(selected_points - center, axis=1)\n",
    "        if np.any(distances == 0):\n",
    "            idx_zero = np.where(distances == 0)[0][0]\n",
    "            channel_x[i] = selected_values[idx_zero, 0]\n",
    "            channel_y[i] = selected_values[idx_zero, 1]\n",
    "            channel_z[i] = selected_values[idx_zero, 2]\n",
    "        else:\n",
    "            weights = 1 / distances\n",
    "            weights /= weights.sum()\n",
    "            channel_x[i] = np.dot(weights, selected_values[:, 0])\n",
    "            channel_y[i] = np.dot(weights, selected_values[:, 1])\n",
    "            channel_z[i] = np.dot(weights, selected_values[:, 2])\n",
    "    \n",
    "    channel_values = np.stack((channel_x, channel_y, channel_z), axis=-1)   \n",
    "    tensor_shape = (num_partitions, num_partitions, num_partitions, 3)\n",
    "    tensor = channel_values.reshape(tensor_shape)\n",
    "\n",
    "    center_values = np.stack((Xc.ravel(), Yc.ravel(), Zc.ravel()), axis=-1)   \n",
    "    center_tensor = center_values.reshape(tensor_shape)\n",
    "    \n",
    "    # Resample tensor to fixed grid size using nearest neighbor interpolation\n",
    "\n",
    "    zoom_factors = [fixed_size / float(orig_size) for fixed_size, orig_size in zip(fixed_grid_size, tensor.shape[:3])]\n",
    "    # Apply zoom with order=0 for nearest neighbor interpolation\n",
    "    tensor_resized = zoom(tensor, zoom_factors + [1], order=0)\n",
    "    tensor_resized = torch.from_numpy(tensor_resized).float()\n",
    "\n",
    "    zoom_factors1 = [fixed_size / float(orig_size) for fixed_size, orig_size in zip(fixed_grid_size, center_tensor.shape[:3])]\n",
    "    tensor_resized1 = zoom(center_tensor, zoom_factors1 + [1], order=0)\n",
    "    center_tensor_resized = torch.from_numpy(tensor_resized1).float()\n",
    "\n",
    "    return (tensor_resized, center_tensor_resized, t, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32efeed5-d6d6-4a34-838f-e0b0ed53292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_points_parallel(\n",
    "    sample_all_data, sample_all_data_point, all_data, all_data_point, epsilon, batch_size=32, save_dir='tensor_batches', fixed_grid_size=(32, 32, 32)\n",
    "):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    N = sample_all_data.shape[0]\n",
    "    tree = cKDTree(all_data)\n",
    "\n",
    "    args_list = [\n",
    "        (i, sample_all_data, sample_all_data_point, all_data, all_data_point, epsilon, tree, fixed_grid_size) for i in range(N)\n",
    "    ]\n",
    "    batch = []\n",
    "    batch_idx = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for arg in tqdm(args_list, total=N, desc='Processing'):\n",
    "        result = process_point(arg)\n",
    "        if result is not None:\n",
    "            tensor, center_tensor, t, idx = result\n",
    "            batch.append((tensor, center_tensor, t, idx))\n",
    "            batch_idx += 1\n",
    "            if batch_idx >= batch_size:\n",
    "                save_path = os.path.join(save_dir, f'batch_{num_batches}.pt')\n",
    "                torch.save(batch, save_path)\n",
    "                batch = []\n",
    "                batch_idx = 0\n",
    "                num_batches += 1\n",
    "\n",
    "    # Save any remaining tensors in the final batch\n",
    "    if batch:\n",
    "        save_path = os.path.join(save_dir, f'batch_{num_batches}.pt')\n",
    "        torch.save(batch, save_path)\n",
    "        num_batches += 1\n",
    "\n",
    "    return num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17d0b8d7-196b-4cad-bd27-edf7d78d8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Dataset to load the saved batches\n",
    "class train_VoxelDataset(Dataset):\n",
    "    def __init__(self, save_dir='tensor_batches/train'):\n",
    "        self.save_dir = save_dir\n",
    "        self.batch_files = [\n",
    "            os.path.join(save_dir, f)\n",
    "            for f in os.listdir(save_dir)\n",
    "            if f.endswith('.pt')\n",
    "        ]\n",
    "        self.batch_files.sort()\n",
    "        self.index_map = []\n",
    "        self._create_index_map()\n",
    "\n",
    "    def _create_index_map(self):\n",
    "        for batch_file in self.batch_files:\n",
    "            batch = torch.load(batch_file)\n",
    "            batch_size = len(batch)\n",
    "            for i in range(batch_size):\n",
    "                self.index_map.append((batch_file, i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_file, tensor_idx = self.index_map[idx]\n",
    "        batch = torch.load(batch_file)\n",
    "        tensor, center_tensor, t, point_idx = batch[tensor_idx]\n",
    "        return tensor, center_tensor, t  # Return both velocity, location and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c55862e-576a-4812-bac7-d59085e8260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Dataset to load the saved batches\n",
    "class valid_VoxelDataset(Dataset):\n",
    "    def __init__(self, save_dir='tensor_batches/valid'):\n",
    "        self.save_dir = save_dir\n",
    "        self.batch_files = [\n",
    "            os.path.join(save_dir, f)\n",
    "            for f in os.listdir(save_dir)\n",
    "            if f.endswith('.pt')\n",
    "        ]\n",
    "        self.batch_files.sort()\n",
    "        self.index_map = []\n",
    "        self._create_index_map()\n",
    "\n",
    "    def _create_index_map(self):\n",
    "        for batch_file in self.batch_files:\n",
    "            batch = torch.load(batch_file)\n",
    "            batch_size = len(batch)\n",
    "            for i in range(batch_size):\n",
    "                self.index_map.append((batch_file, i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_file, tensor_idx = self.index_map[idx]\n",
    "        batch = torch.load(batch_file)\n",
    "        tensor, center_tensor, t, point_idx = batch[tensor_idx]\n",
    "        return tensor, center_tensor, t  # Return both tensor and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f38b2c7-787d-4946-baa5-510e45ffce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract data from vtk file\n",
    "\n",
    "def vtk_to_numpy_array(vtk_array):\n",
    "    return vtk.util.numpy_support.vtk_to_numpy(vtk_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e05c5f4c-92fd-4c90-8871-fde224d0b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_function(func_name):\n",
    "    \"\"\"\n",
    "    Returns a callable for the requested true function g.\n",
    "    The callable should map any input tensor x to another tensor of the same shape.\n",
    "    \"\"\"\n",
    "    if func_name == \"softplus\":\n",
    "        return lambda x: nn.Softplus()(x)\n",
    "    elif func_name == \"square\":\n",
    "        # Just an example: ReLU the input, then square, then divide\n",
    "        return lambda x: (F.relu(x)).pow(2) / 20\n",
    "    elif func_name == \"log\":\n",
    "        # Example piecewise log function\n",
    "        return lambda x: (x/3 + np.log(3) - 2/3)*(x <= 2) + \\\n",
    "                         (torch.log(1 + x*(x > 2)))*(x > 2) \n",
    "    elif func_name == \"cubic\":\n",
    "        return lambda x: x.pow(3)/30\n",
    "    elif func_name == 'None':\n",
    "        return lambda x: x\n",
    "    else:\n",
    "        # Identity if unknown\n",
    "        return lambda x: x\n",
    "\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, images, locs, T, sigma_t_list, true_function='None'):\n",
    "        \"\"\"\n",
    "        images: List of Tensors, each Tensor is shape (C, D, H, W).\n",
    "        T: Number of downsampling steps\n",
    "        sigma_t_list: list of length T (noise scales)\n",
    "        true_function: string or callable. If string, we map it via get_true_function.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.images = images\n",
    "        self.T = T\n",
    "        self.locs = locs\n",
    "        self.sigma_t_list = sigma_t_list\n",
    "        \n",
    "        # Convert the true_function string to an actual callable\n",
    "        if isinstance(true_function, str):\n",
    "            self.g = get_true_function(true_function)\n",
    "        else:\n",
    "            self.g = true_function  # assume user passed a callable directly\n",
    "\n",
    "        self.data_pairs = []\n",
    "        self.prepare_data()\n",
    "\n",
    "    def gaussian_kernel_1d(self, kernel_size, sigma):\n",
    "        # Create a 1D Gaussian kernel\n",
    "        x = torch.arange(kernel_size) - kernel_size // 2\n",
    "        kernel = torch.exp(-0.5 * (x / sigma) ** 2)\n",
    "        kernel = kernel / kernel.sum()\n",
    "        return kernel\n",
    "\n",
    "    def gaussian_blur_3d(self, x, kernel_size=5, sigma=1):\n",
    "        # x: Tensor of shape (C, D, H, W)\n",
    "        device = x.device\n",
    "        x = x.unsqueeze(0)  # batch dimension\n",
    "        N, C, D, H, W = x.shape\n",
    "\n",
    "        # Adjust kernel_size if needed\n",
    "        max_kernel_size = min(kernel_size, D, H, W)\n",
    "        if max_kernel_size % 2 == 0:\n",
    "            max_kernel_size -= 1  # ensure odd\n",
    "        if max_kernel_size < 1:\n",
    "            x_blur = x\n",
    "        else:\n",
    "            kernel = self.gaussian_kernel_1d(max_kernel_size, sigma).to(device)\n",
    "            kernel_3d = kernel[:, None, None] * kernel[None, :, None] * kernel[None, None, :]\n",
    "            kernel_3d = kernel_3d / kernel_3d.sum()\n",
    "            kernel_3d = kernel_3d.view(1, 1, max_kernel_size, max_kernel_size, max_kernel_size)\n",
    "            kernel_3d = kernel_3d.repeat(C, 1, 1, 1, 1)\n",
    "            padding = max_kernel_size // 2\n",
    "            # reflect-pad\n",
    "            x_padded = F.pad(x, (padding, padding, padding, padding, padding, padding), mode='reflect')\n",
    "            x_blur = F.conv3d(x_padded, kernel_3d, groups=C)\n",
    "        return x_blur.squeeze(0)\n",
    "\n",
    "    def interpolate_3d(self, tensor, **kwargs):\n",
    "        # shape: (C, D, H, W)\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "        tensor_interp = F.interpolate(tensor, **kwargs)\n",
    "        return tensor_interp.squeeze(0)\n",
    "\n",
    "    def generate_downsampled_images(self, image, loc):\n",
    "        \"\"\"\n",
    "        image: (C, D, H, W)\n",
    "        Returns a list [X_0, X_1, ..., X_T], \n",
    "        where X_0 is the final smallest scale, X_T is the original resolution.\n",
    "        \"\"\"\n",
    "        X_t_list = []\n",
    "        X_t = image  # start from the highest resolution\n",
    "        X_t_list.append(X_t)\n",
    "\n",
    "        X_loc_list = []\n",
    "        X_loc = loc  \n",
    "        X_loc_list.append(X_loc)\n",
    "        \n",
    "        for t in range(self.T, 0, -1):\n",
    "            # 1. Downsample to half size\n",
    "            X_t_down = self.interpolate_3d(X_t, scale_factor=0.5, \n",
    "                                           mode='trilinear', \n",
    "                                           align_corners=False,\n",
    "                                           recompute_scale_factor=True)\n",
    "            # 2. Blur\n",
    "            X_t_blur = self.gaussian_blur_3d(X_t_down)\n",
    "            # 3. Upsample back to original size\n",
    "            original_size = image.shape[1:]  # (D, H, W)\n",
    "            X_t_blur_upsampled = self.interpolate_3d(X_t_blur, size=original_size,\n",
    "                                                     mode='trilinear', align_corners=False)\n",
    "            X_t_list.insert(0, X_t_blur_upsampled)\n",
    "            X_t = X_t_down\n",
    "\n",
    "            X_loc_list.insert(0, loc)\n",
    "            X_loc = loc\n",
    "        return X_t_list, X_loc_list\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Creates pairs ( (X_{t-1} + epsilon_{t-1}), X_t ) for t in [1..T].\n",
    "        But we also apply g(...) to the noisy input if desired.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.images)):\n",
    "            X_t_list, X_loc_list = self.generate_downsampled_images(self.images[i], self.locs[i])\n",
    "            for t in range(1, self.T + 1):         \n",
    "                X_t = X_t_list[t]\n",
    "                X_loc = X_loc_list[t]\n",
    "                X_t_minus_1 = X_t_list[t - 1]\n",
    "                sigma_t_minus_1 = self.sigma_t_list[t - 1]                \n",
    "                # Noise\n",
    "                epsilon_t_minus_1 = torch.randn_like(X_t_minus_1) * (sigma_t_minus_1 ** 0.5)\n",
    "                X_t_minus_1_noisy = X_t_minus_1 + epsilon_t_minus_1\n",
    "                # APPLY the user-specified g(...) to the noised input\n",
    "                X_t_pred = self.g(X_t_minus_1_noisy)\n",
    "                # We'll store (g-noisy, target, t) in data_pairs\n",
    "                self.data_pairs.append((X_t_pred, X_t, X_loc, t))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_input, X_target, X_coords, t = self.data_pairs[idx]\n",
    "        return X_input, X_target, X_coords, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d02768b-fe57-4470-8a05-d0b2b78f0e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function\n",
    "def custom_collate_fn(batch):\n",
    "    X_inputs, X_targets, X_coords, ts = zip(*batch)\n",
    "    # Determine the maximum spatial dimensions in the batch\n",
    "    max_C = max(x_input.shape[0] for x_input in X_inputs)\n",
    "    max_D = max(x_input.shape[1] for x_input in X_inputs)\n",
    "    max_H = max(x_input.shape[2] for x_input in X_inputs)\n",
    "    max_W = max(x_input.shape[3] for x_input in X_inputs)\n",
    "\n",
    "    # Pad all tensors to the maximum size\n",
    "    X_inputs_padded = []\n",
    "    X_targets_padded = []\n",
    "    X_coords_padded = []\n",
    "    for x_input, x_target, x_coords in zip(X_inputs, X_targets, X_coords):\n",
    "        padding_input = (\n",
    "            0, max_W - x_input.shape[3],  # Width padding\n",
    "            0, max_H - x_input.shape[2],  # Height padding\n",
    "            0, max_D - x_input.shape[1],  # Depth padding\n",
    "        )\n",
    "        padding_target = (\n",
    "            0, max_W - x_target.shape[3],\n",
    "            0, max_H - x_target.shape[2],\n",
    "            0, max_D - x_target.shape[1],\n",
    "        )\n",
    "        padding_coords = (\n",
    "            0, max_W - x_coords.shape[3],\n",
    "            0, max_H - x_coords.shape[2],\n",
    "            0, max_D - x_coords.shape[1],\n",
    "        )\n",
    "        x_input_padded = F.pad(x_input, padding_input, mode='constant', value=0)\n",
    "        x_target_padded = F.pad(x_target, padding_target, mode='constant', value=0)\n",
    "        x_coords_padded = F.pad(x_coords, padding_coords, mode='constant', value=0)\n",
    "        \n",
    "        X_inputs_padded.append(x_input_padded)\n",
    "        X_targets_padded.append(x_target_padded)\n",
    "        X_coords_padded.append(x_coords_padded)\n",
    "\n",
    "    X_inputs_batch = torch.stack(X_inputs_padded)\n",
    "    X_targets_batch = torch.stack(X_targets_padded)\n",
    "    X_coords_batch = torch.stack(X_coords_padded)\n",
    "    ts_batch = torch.tensor(ts)\n",
    "    return X_inputs_batch, X_targets_batch, X_coords_batch, ts_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3524ddc9-4d34-42cb-bcc4-efc18be12c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patient = ['ANY-011-001']\n",
    "valid_patient = ['ANY-035-001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "944d5042-15c5-40fb-850a-10ccbeacb1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.001  # Define the epsilon radius\n",
    "batch_size = 64  # Define the batch size\n",
    "fixed_grid_size = (16, 16, 16)  # Define the fixed grid size for resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ddc6dacf-c05b-4ad1-9081-5234e386e453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 30.32it/s]\n",
      "C:\\Users\\wenxi\\AppData\\Local\\Temp\\ipykernel_96036\\3457695896.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  batch = torch.load(batch_file)\n",
      "C:\\Users\\wenxi\\AppData\\Local\\Temp\\ipykernel_96036\\3457695896.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  batch = torch.load(batch_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2 valid batches of tensors.\n",
      "Batch tensors shape: torch.Size([64, 16, 16, 16, 3])\n",
      "Batch centers shape: torch.Size([64, 16, 16, 16, 3])\n",
      "Unique batch t values: tensor([3, 4])\n",
      "Batch tensors shape: torch.Size([36, 16, 16, 16, 3])\n",
      "Batch centers shape: torch.Size([36, 16, 16, 16, 3])\n",
      "Unique batch t values: tensor([2, 3, 4])\n",
      "Loading images from processed tensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading batches:   0%|                                                                           | 0/2 [00:00<?, ?it/s]C:\\Users\\wenxi\\AppData\\Local\\Temp\\ipykernel_96036\\74604903.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  batch = torch.load(batch_file)  # Each batch is a list of (tensor, t, idx)\n",
      "Loading batches: 100%|███████████████████████████████████████████████████████████████████████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "train_input_list = []\n",
    "train_target_list = []\n",
    "train_loc_list = []\n",
    "train_t_list = []\n",
    "\n",
    "for i in range(len(train_patient)):\n",
    "    filename = train_patient[i] + str('_00.vtm')\n",
    "    path = '/Data/' + filename\n",
    "    \n",
    "    reader = vtk.vtkXMLMultiBlockDataReader()\n",
    "    reader.SetFileName(path)  ## change here for other data sets\n",
    "    reader.Update()\n",
    "    \n",
    "    blocks = [reader.GetOutput().GetBlock(i) for i in range(reader.GetOutput().GetNumberOfBlocks())]\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    points_list = []\n",
    "    point_data_list = []\n",
    "    \n",
    "    for block in blocks:\n",
    "        if isinstance(block, vtk.vtkUnstructuredGrid):\n",
    "            points = vtk_to_numpy_array(block.GetPoints().GetData())\n",
    "            point_data = vtk_to_numpy_array(block.GetPointData().GetArray('velocity'))\n",
    "    \n",
    "            points_list.append(points)\n",
    "            point_data_list.append(point_data)\n",
    "\n",
    "    # Convert lists to single numpy arrays\n",
    "    all_points = np.vstack(points_list)\n",
    "    all_point_data = np.vstack(point_data_list)\n",
    "    \n",
    "    # Create a mask for selecting rows where not all dimensions are zero\n",
    "    mask = np.any(all_point_data != 0, axis=1)\n",
    "    \n",
    "    # Apply the mask to filter the data\n",
    "    filtered_all_point_data = all_point_data[mask]\n",
    "    \n",
    "    filtered_all_points = all_points[mask]\n",
    "    \n",
    "    # Fix a random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    # Number of samples to subsample\n",
    "    n_samples = 100\n",
    "    \n",
    "    # Generate random indices\n",
    "    indices = np.random.choice(filtered_all_points.shape[0], size=n_samples, replace=False)\n",
    "    \n",
    "    # Subsample data\n",
    "    all_points_subsampled = filtered_all_points[indices]\n",
    "    all_point_data_subsampled = filtered_all_point_data[indices]\n",
    "    \n",
    "    # Process all points in parallel and save tensors in batches with progress bar\n",
    "    save_dir = 'tensor_batches/train/' + train_patient[i]\n",
    "    \n",
    "    train_num_batches = process_all_points_parallel(\n",
    "        all_points_subsampled, all_point_data_subsampled, filtered_all_points, filtered_all_point_data, epsilon, batch_size=batch_size, save_dir=save_dir, fixed_grid_size=fixed_grid_size\n",
    "    )\n",
    "    print(f\"Saved {train_num_batches} valid batches of tensors.\")\n",
    "    \n",
    "    # Create a Dataset and DataLoader for PyTorch\n",
    "    train_dataset = train_VoxelDataset(save_dir=save_dir)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Iterate over the dataloader\n",
    "    for batch_tensors, batch_center, batch_t_values in train_dataloader:\n",
    "        print(f\"Batch tensors shape: {batch_tensors.shape}\")\n",
    "        print(f\"Batch centers shape: {batch_tensors.shape}\")\n",
    "        print(f\"Unique batch t values: {batch_t_values.unique()}\")\n",
    "\n",
    "    # List all tensor batch files\n",
    "    batch_files = [\n",
    "            os.path.join(save_dir, f)\n",
    "            for f in os.listdir(save_dir)\n",
    "            if f.endswith('.pt')]\n",
    "        \n",
    "    batch_files.sort()\n",
    "    \n",
    "    images = []\n",
    "    locs = []\n",
    "    \n",
    "    print(\"Loading images from processed tensors...\")\n",
    "    for batch_file in tqdm(batch_files, desc=\"Loading batches\"):\n",
    "        batch = torch.load(batch_file)  # Each batch is a list of (tensor, t, idx)\n",
    "        for data in batch:\n",
    "            tensor, center, t, idx = data\n",
    "            # tensor is of shape (D, H, W, C), need to permute to (C, D, H, W)\n",
    "            tensor = tensor.permute(3, 0, 1, 2)\n",
    "            center = center.permute(3, 0, 1, 2)\n",
    "            images.append(tensor)\n",
    "            locs.append(center)\n",
    "    \n",
    "    \n",
    "    T = 4  # Maximum resolution level\n",
    "    sigma_t_list = [0.1] * T  # Prespecified \\sigma_t^2 for each t from 0 to T-1\n",
    "    \n",
    "    train_dataset = SuperResolutionDataset(images, locs, T, sigma_t_list)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "    for X_input, X_target, X_coords, t in train_dataloader:\n",
    "        train_input_list.append(X_input)\n",
    "        train_target_list.append(X_target)\n",
    "        train_loc_list.append(X_coords)\n",
    "        train_t_list.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1dfcf686-c22b-4456-89e7-8a67c45e2238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 16.53it/s]\n",
      "C:\\Users\\wenxi\\AppData\\Local\\Temp\\ipykernel_96036\\860782534.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  batch = torch.load(batch_file)\n",
      "C:\\Users\\wenxi\\AppData\\Local\\Temp\\ipykernel_96036\\860782534.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  batch = torch.load(batch_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2 valid batches of tensors.\n",
      "Batch tensors shape: torch.Size([64, 16, 16, 16, 3])\n",
      "Unique batch t values: tensor([2, 3, 4])\n",
      "Batch tensors shape: torch.Size([36, 16, 16, 16, 3])\n",
      "Unique batch t values: tensor([2, 3, 4])\n",
      "Loading images from processed tensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading batches:   0%|                                                                           | 0/2 [00:00<?, ?it/s]C:\\Users\\wenxi\\AppData\\Local\\Temp\\ipykernel_96036\\2960823907.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  batch = torch.load(batch_file)  # Each batch is a list of (tensor, t, idx)\n",
      "Loading batches: 100%|███████████████████████████████████████████████████████████████████████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_input_list = []\n",
    "valid_target_list = []\n",
    "valid_loc_list = []\n",
    "\n",
    "for i in range(len(valid_patient)):\n",
    "    filename = valid_patient[i] + str('_00.vtm')\n",
    "    path = '/Data/' + filename\n",
    "    \n",
    "    reader = vtk.vtkXMLMultiBlockDataReader()\n",
    "    reader.SetFileName(path)  ## change here for other data sets\n",
    "    reader.Update()\n",
    "    \n",
    "    blocks = [reader.GetOutput().GetBlock(i) for i in range(reader.GetOutput().GetNumberOfBlocks())]\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    points_list = []\n",
    "    point_data_list = []\n",
    "    \n",
    "    for block in blocks:\n",
    "        if isinstance(block, vtk.vtkUnstructuredGrid):\n",
    "            points = vtk_to_numpy_array(block.GetPoints().GetData())\n",
    "            point_data = vtk_to_numpy_array(block.GetPointData().GetArray('velocity'))\n",
    "    \n",
    "            points_list.append(points)\n",
    "            point_data_list.append(point_data)\n",
    "       \n",
    "    # Convert lists to single numpy arrays\n",
    "    all_points = np.vstack(points_list)\n",
    "    all_point_data = np.vstack(point_data_list)\n",
    "    \n",
    "    # Create a mask for selecting rows where not all dimensions are zero\n",
    "    mask = np.any(all_point_data != 0, axis=1)\n",
    "    \n",
    "    # Apply the mask to filter the data\n",
    "    filtered_all_point_data = all_point_data[mask]\n",
    "    \n",
    "    filtered_all_points = all_points[mask]\n",
    "    \n",
    "    # Fix a random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    # Number of samples to subsample\n",
    "    n_samples = 100\n",
    "    \n",
    "    # Generate random indices\n",
    "    indices = np.random.choice(filtered_all_points.shape[0], size=n_samples, replace=False)\n",
    "    \n",
    "    # Subsample data\n",
    "    all_points_subsampled = filtered_all_points[indices]\n",
    "    all_point_data_subsampled = filtered_all_point_data[indices]\n",
    "    \n",
    "    # Process all points in parallel and save tensors in batches with progress bar\n",
    "    save_dir = 'tensor_batches/valid/' + valid_patient[i]\n",
    "    \n",
    "    valid_num_batches = process_all_points_parallel(\n",
    "        all_points_subsampled, all_point_data_subsampled, filtered_all_points, filtered_all_point_data, epsilon, batch_size=batch_size, save_dir=save_dir, fixed_grid_size=fixed_grid_size\n",
    "    )\n",
    "    print(f\"Saved {valid_num_batches} valid batches of tensors.\")\n",
    "    \n",
    "    # Create a Dataset and DataLoader for PyTorch\n",
    "    valid_dataset = valid_VoxelDataset(save_dir=save_dir)\n",
    "    \n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Iterate over the dataloader\n",
    "    for batch_tensors, batch_center, batch_t_values in valid_dataloader:\n",
    "        print(f\"Batch tensors shape: {batch_tensors.shape}\")\n",
    "        print(f\"Unique batch t values: {batch_t_values.unique()}\")\n",
    "\n",
    "    # List all tensor batch files\n",
    "    batch_files = [\n",
    "            os.path.join(save_dir, f)\n",
    "            for f in os.listdir(save_dir)\n",
    "            if f.endswith('.pt')]\n",
    "        \n",
    "    batch_files.sort()\n",
    "\n",
    "    images = []\n",
    "    locs = []\n",
    "    \n",
    "    print(\"Loading images from processed tensors...\")\n",
    "    for batch_file in tqdm(batch_files, desc=\"Loading batches\"):\n",
    "        batch = torch.load(batch_file)  # Each batch is a list of (tensor, t, idx)\n",
    "        for data in batch:\n",
    "            tensor, center, t, idx = data\n",
    "            # tensor is of shape (D, H, W, C), need to permute to (C, D, H, W)\n",
    "            tensor = tensor.permute(3, 0, 1, 2)\n",
    "            center = center.permute(3, 0, 1, 2)\n",
    "            images.append(tensor)\n",
    "            locs.append(center)\n",
    "    \n",
    "    \n",
    "    T = 4  # Maximum resolution level\n",
    "    sigma_t_list = [0] * T  # Prespecified \\sigma_t^2 for each t from 0 to T-1\n",
    "    \n",
    "    valid_dataset = SuperResolutionDataset(images, locs, T, sigma_t_list)\n",
    "\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "    for X_input, X_target, X_coords, t in valid_dataloader:\n",
    "        valid_input_list.append(X_input)\n",
    "        valid_target_list.append(X_target)\n",
    "        valid_loc_list.append(X_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e89b7bc-d672-44f9-94d1-09c5e686a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_input = torch.cat(valid_input_list, dim = 0)\n",
    "valid_target = torch.cat(valid_target_list, dim = 0)\n",
    "valid_loc = torch.cat(valid_loc_list, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ff0a60a-b7bc-43dd-b7d4-bf7670fe10d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 3, 16, 16, 16])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bdebf0b3-d31d-4fa4-bd48-0b689403e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = torch.cat(train_input_list, dim = 0)\n",
    "train_target = torch.cat(train_target_list, dim = 0)\n",
    "train_loc = torch.cat(train_loc_list, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bdcd97c4-e852-43ce-8386-4fd90d02b1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 3, 16, 16, 16])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95d1b8fe-3541-4fff-a3f7-3602019a577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_input, \"/Data/train_input_100.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc6307dc-5aaa-4b09-9e3d-c7a173578449",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_target, \"/Data/train_target_100.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fb02ea8b-aac8-4404-8839-a3cd334551cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(valid_input, \"/Data/valid_input_100.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b4d63aac-098b-4823-9c52-0310628718f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(valid_target, \"/Data/valid_target_100.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd33aa-00a2-44a2-92fb-487afb75f79a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
