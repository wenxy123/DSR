{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d33018f-f87f-43f2-a4ca-f37f1ab0850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split, SubsetRandomSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.ndimage import zoom  # For resampling\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.linalg import vector_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41972489-454f-4171-a8a1-32a5cccb18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DSR_finetune import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d050a0b-7ce4-4a64-b231-48946679e6d7",
   "metadata": {},
   "source": [
    "# Data preparation for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b490d-7441-45ee-8b7e-dd6f8e92be3b",
   "metadata": {},
   "source": [
    "For the fine-tuning process, we use both 4DF and CFD data. When these data are stored as 3D velocity fields with corresponding spatial coordinates (as in the pretraining stage), cubic patches can be constructed using the `pretrain_data_preparation' procedure. Below, we provide the code for the construction of the cubic patch where the 4DF and CFD data are stored in the format (m, n, k, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ca83da-e5a0-4b65-8e7a-d9bca1c05213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .mat file\n",
    "vcdf_mat = scipy.io.loadmat(\"data/CFD4DF/VCFD.mat\")\n",
    "Velocity_mat = scipy.io.loadmat(\"data/CFD4DF/Velocity.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3864d632-0678-4423-98ce-3a76d1cb2228",
   "metadata": {},
   "outputs": [],
   "source": [
    "vcdf_data = vcdf_mat['Velocity_CFD'][0]\n",
    "vel_data = Velocity_mat['Velocity'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0becc9d-7f79-434d-8d30-0993d68a50dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 901, 231, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcdf_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aacf59e2-3284-4771-8d75-f616532bb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vcdf_data_flat = vcdf_data.reshape(-1, 3)\n",
    "\n",
    "# Create a boolean mask that is True for rows that are not all zeros.\n",
    "mask1 = ~np.all(vcdf_data_flat == 0, axis=1)\n",
    "mask0 = np.all(vcdf_data_flat == 0, axis=1)\n",
    "vcdf_data1 = vcdf_data_flat[mask1]\n",
    "\n",
    "x = 46\n",
    "y = 46\n",
    "z = 168\n",
    "\n",
    "vcdf_data2 = vcdf_data1.reshape(x, y, z, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3272c2b1-b0e8-4caf-aee5-f18b775f3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_data_flat = vel_data.reshape(-1, 3)\n",
    "\n",
    "# Create a boolean mask that is True for rows that are not all zeros.\n",
    "vel_data1 = vel_data_flat[mask1]\n",
    "\n",
    "x = 46\n",
    "y = 46\n",
    "z = 168\n",
    "\n",
    "vel_data2 = vel_data1.reshape(x, y, z, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "540b0224-8fd8-4c25-b7b4-b0d91218fbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded data1 shape: torch.Size([48, 48, 176, 3])\n",
      "Expanded data2 shape: torch.Size([48, 48, 176, 3])\n"
     ]
    }
   ],
   "source": [
    "data_input = torch.Tensor(vel_data2)\n",
    "data_target = torch.Tensor(vcdf_data2)\n",
    "\n",
    "pad_tuple = (0, 0,  # +0 on dim #3 -> remains 3\n",
    "             4, 4,  # +8 on dim #2 -> 168 + 8 = 176\n",
    "             1, 1,  # +2 on dim #1 -> 46 + 2 = 48\n",
    "             1, 1)  # +2 on dim #0 -> 46 + 2 = 48\n",
    "\n",
    "# Apply padding - remove torch.Tensor() wrapper from pad_tuple\n",
    "data_input_expanded = F.pad(data_input, pad_tuple)\n",
    "data_target_expanded = F.pad(data_target, pad_tuple)\n",
    "\n",
    "# Verify the new shape\n",
    "print(\"Expanded data1 shape:\", data_input_expanded.shape)\n",
    "print(\"Expanded data2 shape:\", data_target_expanded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1929bc06-1b4e-4d28-b06a-53d7376a258a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_inputs_tensor shape: torch.Size([99, 16, 16, 16, 3])\n",
      "X_targets_tensor shape: torch.Size([99, 16, 16, 16, 3])\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Generate cubic patches of shape [16, 16, 16, 3] from both data1 and data2\n",
    "patch_size = 16\n",
    "\n",
    "# Prepare lists to store the patches for both data1 (X_inputs) and data2 (X_targets)\n",
    "X_inputs_patches = []\n",
    "X_targets_patches = []\n",
    "\n",
    "# Loop through the expanded data to extract cubic patches\n",
    "for i in range(0, data_input_expanded.shape[0], patch_size):  # Loop along the first dimension (80)\n",
    "    for j in range(0, data_input_expanded.shape[1], patch_size):  # Loop along the second dimension (912)\n",
    "        for k in range(0, data_input_expanded.shape[2], patch_size):  # Loop along the third dimension (240)\n",
    "            # Extract patches from data1 (X_inputs) and data2 (X_targets)\n",
    "            patch_data1 = data_input_expanded[i:i+patch_size, j:j+patch_size, k:k+patch_size, :]\n",
    "            patch_data2 = data_target_expanded[i:i+patch_size, j:j+patch_size, k:k+patch_size, :]\n",
    "            \n",
    "            if patch_data1.shape == torch.Size([patch_size, patch_size, patch_size, 3]):  # Ensure the patch is correct\n",
    "                X_inputs_patches.append(patch_data1)\n",
    "                X_targets_patches.append(patch_data2)\n",
    "\n",
    "# Convert the list of patches into tensors\n",
    "X_inputs_tensor = torch.stack(X_inputs_patches)\n",
    "X_targets_tensor = torch.stack(X_targets_patches)\n",
    "\n",
    "# Verify the shape of the patches tensors\n",
    "print(\"X_inputs_tensor shape:\", X_inputs_tensor.shape)  # Should be [num_patches, 16, 16, 16, 3]\n",
    "print(\"X_targets_tensor shape:\", X_targets_tensor.shape)  # Should be [num_patches, 16, 16, 16, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d7a5b6d-d5ba-4460-9a31-8f783c452cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_inputs_tensor1 = torch.permute(X_inputs_tensor, (0, 4, 1, 2, 3))\n",
    "X_targets_tensor1 = torch.permute(X_targets_tensor, (0, 4, 1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02e33e79-a925-4cab-af96-a00a49e27d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62 40 95 18 97 84 64 42 10  0 31 76 47 26 44]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 15\n",
    "\n",
    "# Generate random indices\n",
    "indices = np.random.choice(X_inputs_tensor1.shape[0], size=n_samples, replace=False)\n",
    "print(indices)\n",
    "\n",
    "# Subsample data\n",
    "train_x_input = X_inputs_tensor1[indices]\n",
    "train_x_target = X_targets_tensor1[indices]\n",
    "\n",
    "N = X_inputs_tensor1.shape[0]\n",
    "\n",
    "mask = np.ones(N, dtype=bool)\n",
    "\n",
    "mask[indices] = False\n",
    "\n",
    "test_x_input = X_inputs_tensor1[mask]\n",
    "test_x_target = X_targets_tensor1[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c1841c-4cdb-4d59-9e88-4deb20b89597",
   "metadata": {},
   "source": [
    "# Two-step fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "760427a1-6e5f-48ca-aae6-c6eeb3103516",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'code/DSR_code/DSR/DSR_unet_model_pretrain.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21bfd4a8-fb5c-4d98-9a55-8d0b2637ff66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 28.3747,  E(|Y-Yhat|): 42.8573,  E(|Yhat-Yhat'|): 28.9652\n",
      "[Epoch 10 (30%)] energy-loss: 15.2043,  E(|Y-Yhat|): 26.9655,  E(|Yhat-Yhat'|): 23.5224\n",
      "[Epoch 20 (63%)] energy-loss: 12.9381,  E(|Y-Yhat|): 22.0781,  E(|Yhat-Yhat'|): 18.2802\n",
      "[Epoch 30 (97%)] energy-loss: 11.5291,  E(|Y-Yhat|): 24.6507,  E(|Yhat-Yhat'|): 26.2431\n"
     ]
    }
   ],
   "source": [
    "dsr_finetune1 = dsr_fine_tune_step1(train_x_input, train_x_target, \n",
    "                                  load_path=model_path,\n",
    "                                  lr=1e-5,  \n",
    "                                  num_epochs=30,\n",
    "                                  print_every_nepoch=10,\n",
    "                                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df828b50-a753-4fc6-b512-80c2c683e454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved with validation loss inf\n"
     ]
    }
   ],
   "source": [
    "dsr_finetune1.save_model(\"code/DSR_code/DSR/DSR_unet_model_finetune_s1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2f10d56-a38e-4f14-9762-4d19e0022d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path1 = 'code/DSR_code/DSR/DSR_unet_model_finetune_s1.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "282a34b6-c5ab-40f8-9b42-a742453da7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 11.7407,  E(|Y-Yhat|): 22.3383,  E(|Yhat-Yhat'|): 21.1951\n",
      "[Epoch 10 (45%)] energy-loss: 10.4731,  E(|Y-Yhat|): 20.7084,  E(|Yhat-Yhat'|): 20.4706\n",
      "[Epoch 20 (95%)] energy-loss: 9.6854,  E(|Y-Yhat|): 19.8234,  E(|Yhat-Yhat'|): 20.2760\n"
     ]
    }
   ],
   "source": [
    "dsr_finetune2 = dsr_fine_tune_step2(train_x_input, train_x_target, \n",
    "                                  load_path=model_path1,\n",
    "                                  lr=1e-4,  \n",
    "                                  num_epochs=20,\n",
    "                                  print_every_nepoch=10,\n",
    "                                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa869123-5b7c-43e5-a0f4-3053e4c937ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved with validation loss inf\n"
     ]
    }
   ],
   "source": [
    "dsr_finetune2.save_model(\"code/DSR_code/DSR/DSR_unet_model_finetune_s2.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738edf69-340d-4da8-8ccc-7d098c48ea43",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e66081be-1cb6-44e9-875d-bf063d7044ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): UNet3D(\n",
       "    (encoder1): Sequential(\n",
       "      (0): Conv3d(3, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout3d(p=0.2, inplace=False)\n",
       "      (4): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (5): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Dropout3d(p=0.2, inplace=False)\n",
       "    )\n",
       "    (pool1): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (encoder2): Sequential(\n",
       "      (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout3d(p=0.2, inplace=False)\n",
       "      (4): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (5): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Dropout3d(p=0.2, inplace=False)\n",
       "    )\n",
       "    (pool2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (encoder3): Sequential(\n",
       "      (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout3d(p=0.2, inplace=False)\n",
       "      (4): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (5): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Dropout3d(p=0.2, inplace=False)\n",
       "    )\n",
       "    (bottleneck): Sequential(\n",
       "      (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout3d(p=0.2, inplace=False)\n",
       "      (4): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (5): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Dropout3d(p=0.2, inplace=False)\n",
       "    )\n",
       "    (upconv3): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (decoder3): Sequential(\n",
       "      (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout3d(p=0.2, inplace=False)\n",
       "      (4): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (5): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Dropout3d(p=0.2, inplace=False)\n",
       "    )\n",
       "    (upconv2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (decoder2): Sequential(\n",
       "      (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout3d(p=0.2, inplace=False)\n",
       "      (4): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (5): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Dropout3d(p=0.2, inplace=False)\n",
       "    )\n",
       "    (upconv1): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (decoder1): Sequential(\n",
       "      (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout3d(p=0.2, inplace=False)\n",
       "      (4): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (5): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Dropout3d(p=0.2, inplace=False)\n",
       "    )\n",
       "    (conv): Identity()\n",
       "  )\n",
       "  (1): AdvancedHead(\n",
       "    (proj): Sequential(\n",
       "      (0): Conv3d(32, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout3d(p=0.1, inplace=False)\n",
       "    )\n",
       "    (res1): ResBlock3D(\n",
       "      (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (res2): ResBlock3D(\n",
       "      (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (spatial_att): SpatialAttention3D(\n",
       "      (conv): Conv3d(2, 1, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), bias=False)\n",
       "    )\n",
       "    (se): Sequential(\n",
       "      (0): AdaptiveAvgPool3d(output_size=1)\n",
       "      (1): Conv3d(128, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv3d(8, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      (4): Sigmoid()\n",
       "    )\n",
       "    (out): Conv3d(128, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('code/DSR_code/DSR/DSR_unet_model_finetune_s2.pth')\n",
    "\n",
    "backbone = UNet3D(in_channels=3, out_channels=3, init_features=32).to(device)\n",
    "backbone.conv = nn.Identity()                          \n",
    "new_head = AdvancedHead(feat_ch=32, mid_ch=128, out_ch=3,\n",
    "                        dropout_p=0.1).to(device)\n",
    "DSR_tune = nn.Sequential(backbone, new_head)\n",
    "DSR_tune.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "DSR_tune.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69faeed7-f043-4cf1-92f3-cd93782129b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DSR prediction\n",
    "batch_size = 32\n",
    "sigma_t = 0.1\n",
    "n_samples = test_x_input.shape[0]\n",
    "\n",
    "num_runs = 200\n",
    "DSR_predictions = []\n",
    "\n",
    "# Use torch.no_grad() to prevent storing gradients during inference.\n",
    "for start_idx in range(0, n_samples, batch_size):\n",
    "    end_idx = min(start_idx + batch_size, n_samples)\n",
    "    \n",
    "    batch = test_x_input[start_idx:end_idx].to(device)\n",
    "    target = test_x_target[start_idx:end_idx].to(device)\n",
    "    batch_sum = torch.zeros_like(target).to(device)\n",
    "\n",
    "    # Loop over the number of runs to compute predictions\n",
    "    for run in range(num_runs):\n",
    "        # Generate noise and add it to the input batch\n",
    "        epsilon_t = torch.randn_like(batch) * (sigma_t ** 0.5)\n",
    "        input_batch = batch + epsilon_t\n",
    "        DSR_preds = DSR_tune(input_batch)\n",
    "        batch_sum += DSR_preds.detach()\n",
    "\n",
    "    # Compute the average prediction for this batch\n",
    "    batch_mean = batch_sum / num_runs\n",
    "    DSR_predictions.append(batch_mean)\n",
    "        \n",
    "# Concatenate all batch predictions into one final prediction tensor\n",
    "DSR_predictions = torch.cat(DSR_predictions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41dfb68b-f81d-4320-983f-0ecd1532bc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([84, 3, 16, 16, 16])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DSR_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20351f95-31ea-4cd5-a6e5-1f65f9a41d23",
   "metadata": {},
   "source": [
    "## Shape reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "153e1b48-dda0-45ef-a5f2-14217d84e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructed_predict(predict_tensor):\n",
    "    patch_size = 16\n",
    "    num_patches_dim0 = 48 // patch_size  \n",
    "    num_patches_dim1 = 48 // patch_size  \n",
    "    num_patches_dim2 = 176 // patch_size  \n",
    "    \n",
    "    predict_tensor = torch.Tensor(predict_tensor).permute(0,2,3,4,1)\n",
    "\n",
    "    predict_reconstructed = predict_tensor.reshape(num_patches_dim0, num_patches_dim1, num_patches_dim2,\n",
    "                                              patch_size, patch_size, patch_size, 3)\n",
    "\n",
    "    predict_reconstructed = predict_reconstructed.permute(0, 3, 1, 4, 2, 5, 6)\n",
    "\n",
    "    predict_reconstructed = predict_reconstructed.reshape(48, num_patches_dim1 * patch_size, num_patches_dim2 * patch_size, 3)\n",
    "    \n",
    "    predict_reconstructed1 = predict_reconstructed[1:-1, 1:-1, 4:-4, :]\n",
    "    \n",
    "    return predict_reconstructed1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7827af2a-b134-4e96-942e-60e8ae8ea576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_origin_shape(prediction):\n",
    "    original_shape = (99, 3, 16, 16, 16)\n",
    "\n",
    "    fine_tuning_data = train_x_target \n",
    "    #fine_tuning_indices = np.array([62, 40, 95, 18, 97, 84, 64, 42, 10])\n",
    "    fine_tuning_indices = np.array([62,40,95,18,97,84,64,42,10,0,31,76,47,26,44])\n",
    "\n",
    "    predictions = prediction \n",
    "\n",
    "    combined_data = np.zeros(original_shape)\n",
    "\n",
    "    combined_data[fine_tuning_indices] = fine_tuning_data\n",
    "\n",
    "    all_indices = np.arange(original_shape[0])  # [0, 1, ..., 98]\n",
    "    remaining_indices = np.setdiff1d(all_indices, fine_tuning_indices)  # 90 indices\n",
    "\n",
    "    combined_data[remaining_indices] = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    eng_reshape1 = reconstructed_predict(combined_data)\n",
    "\n",
    "    eng_reshape1_flat = eng_reshape1.reshape(-1,3)\n",
    "\n",
    "    eng_prepare = np.zeros((15817956, 3))\n",
    "\n",
    "    eng_prepare[mask1] = eng_reshape1_flat.detach().numpy()\n",
    "\n",
    "    eng_reconstructed = eng_prepare.reshape(76, 901, 231, 3)\n",
    "\n",
    "    return eng_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9073453a-8d8f-4488-a0be-4e59beab5345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 901, 231, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DSR_reconstructed = construct_origin_shape(DSR_predictions)\n",
    "DSR_reconstructed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c4033-5e93-4033-87bb-958f6120ed5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
